{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "id": "5ZQR1opMAv6h"
      },
      "id": "5ZQR1opMAv6h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ly_wgYC2Awc",
        "outputId": "8ff14d45-c282-4a7c-cc8a-a89a7607e3a4"
      },
      "id": "5ly_wgYC2Awc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VT1WrpkRQUFV"
      },
      "id": "VT1WrpkRQUFV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbfc90db",
      "metadata": {
        "id": "dbfc90db"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Répertoire source où se trouvent les dossiers\n",
        "source_dir = \"/Users/esmahannachi/downloads\"\n",
        "\n",
        "# Répertoire de destination où vous souhaitez copier le contenu\n",
        "destination_dir = \"/Users/esmahannachi/downloads/IRM\"\n",
        "\n",
        "# Fonction pour fusionner le contenu de deux dossiers\n",
        "def fusionner_dossiers(dossier_source, dossier_destination):\n",
        "    # Liste de tous les éléments (fichiers et dossiers) dans le dossier source\n",
        "    items = os.listdir(dossier_source)\n",
        "\n",
        "    # Parcourir chaque élément dans le dossier source\n",
        "    for item in items:\n",
        "        chemin_item_source = os.path.join(dossier_source, item)\n",
        "        chemin_item_destination = os.path.join(dossier_destination, item)\n",
        "\n",
        "        # Si l'élément est un dossier, fusionner son contenu récursivement\n",
        "        if os.path.isdir(chemin_item_source):\n",
        "            if os.path.exists(chemin_item_destination):\n",
        "                # Si un dossier du même nom existe déjà dans la destination,\n",
        "                # fusionner le contenu des deux dossiers\n",
        "                fusionner_dossiers(chemin_item_source, chemin_item_destination)\n",
        "            else:\n",
        "                # Si le dossier n'existe pas dans la destination, le copier simplement\n",
        "                shutil.copytree(chemin_item_source, chemin_item_destination)\n",
        "        else:\n",
        "            # Si l'élément est un fichier, le copier dans le dossier de destination\n",
        "            # Vérifier si les fichiers source et destination sont les mêmes avant de copier\n",
        "            if not os.path.exists(chemin_item_destination) or os.path.abspath(chemin_item_source) != os.path.abspath(chemin_item_destination):\n",
        "                shutil.copy2(chemin_item_source, dossier_destination)\n",
        "\n",
        "# Liste de tous les dossiers dans le répertoire source\n",
        "dossiers = [f for f in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, f))]\n",
        "\n",
        "# Parcourir chaque dossier\n",
        "for dossier in dossiers:\n",
        "    # Construire le chemin du dossier source\n",
        "    chemin_dossier_source = os.path.join(source_dir, dossier)\n",
        "\n",
        "    # Liste de tous les sous-dossiers dans le dossier actuel\n",
        "    sous_dossiers = [f for f in os.listdir(chemin_dossier_source) if os.path.isdir(os.path.join(chemin_dossier_source, f))]\n",
        "\n",
        "    # Parcourir chaque sous-dossier et copier son contenu dans le répertoire de destination\n",
        "    for sous_dossier in sous_dossiers:\n",
        "        # Construire le chemin du sous-dossier source\n",
        "        chemin_sous_dossier_source = os.path.join(chemin_dossier_source, sous_dossier)\n",
        "\n",
        "        # Construire le chemin du sous-dossier de destination\n",
        "        chemin_sous_dossier_destination = os.path.join(destination_dir, sous_dossier)\n",
        "\n",
        "        if os.path.exists(chemin_sous_dossier_destination):\n",
        "            # Si un dossier du même nom existe déjà dans la destination,\n",
        "            # fusionner le contenu des deux dossiers\n",
        "            fusionner_dossiers(chemin_sous_dossier_source, chemin_sous_dossier_destination)\n",
        "        else:\n",
        "            # Si le dossier n'existe pas dans la destination, le copier simplement\n",
        "            shutil.copytree(chemin_sous_dossier_source, chemin_sous_dossier_destination)\n",
        "\n",
        "print(\"Dossiers copiés et fusionnés avec succès !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83fd0508",
      "metadata": {
        "id": "83fd0508"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def trouver_fichier_nii(dossier):\n",
        "    # Parcourir tous les fichiers et dossiers dans le répertoire donné\n",
        "    for item in os.listdir(dossier):\n",
        "        chemin_item = os.path.join(dossier, item)\n",
        "        if os.path.isdir(chemin_item):\n",
        "            # S'il s'agit d'un dossier, rechercher récursivement le fichier .NII\n",
        "            fichier_nii = trouver_fichier_nii(chemin_item)\n",
        "            if fichier_nii:\n",
        "                return fichier_nii\n",
        "        elif item.endswith('.nii'):\n",
        "            # S'il s'agit d'un fichier .NII, retourner son chemin\n",
        "            return chemin_item\n",
        "    # Si aucun fichier .NII n'est trouvé, retourner None\n",
        "    return None\n",
        "\n",
        "def renommer_et_deplacer_nii(dossier_racine, dossier_sortie):\n",
        "    for dossier_patient in os.listdir(dossier_racine):\n",
        "        chemin_dossier_patient = os.path.join(dossier_racine, dossier_patient)\n",
        "        if os.path.isdir(chemin_dossier_patient):\n",
        "            # Vérifier si le dossier du patient contient des sous-dossiers\n",
        "            sous_dossiers = [f for f in os.listdir(chemin_dossier_patient) if os.path.isdir(os.path.join(chemin_dossier_patient, f))]\n",
        "            if sous_dossiers:\n",
        "                # S'il y a des sous-dossiers, obtenir le fichier .NII du premier sous-dossier\n",
        "                fichier_nii = trouver_fichier_nii(os.path.join(chemin_dossier_patient, sous_dossiers[0]))\n",
        "            else:\n",
        "                # S'il n'y a pas de sous-dossiers, rechercher directement le fichier .NII dans le dossier du patient\n",
        "                fichier_nii = trouver_fichier_nii(chemin_dossier_patient)\n",
        "\n",
        "            if fichier_nii:\n",
        "                # Renommer et déplacer le fichier .NII vers le répertoire de sortie\n",
        "                nouveau_nom = os.path.join(dossier_sortie, f\"{dossier_patient}.nii\")\n",
        "                shutil.move(fichier_nii, nouveau_nom)\n",
        "                print(f\"Déplacé {fichier_nii} vers {nouveau_nom}\")\n",
        "\n",
        "# Répertoire source\n",
        "dossier_source = '/Users/esmahannachi/downloads/IRM'\n",
        "# Répertoire de destination\n",
        "dossier_destination = '/Users/esmahannachi/downloads/IRMIMAGE'\n",
        "\n",
        "renommer_et_deplacer_nii(dossier_source, dossier_destination)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13fa59eb",
      "metadata": {
        "id": "13fa59eb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Lire le fichier CSV d'origine\n",
        "fichier_csv_original = '/Users/esmahannachi/downloads/Esma_4_22_2024.csv'  # Remplacer par le chemin réel de votre fichier CSV\n",
        "df = pd.read_csv(fichier_csv_original)\n",
        "\n",
        "# Sélectionner uniquement les colonnes 'subject' et 'group'\n",
        "colonnes_selectionnees = df[['Subject', 'Group']]\n",
        "\n",
        "# Définir le chemin pour le nouveau fichier CSV\n",
        "nouveau_fichier_csv = '/Users/esmahannachi/downloads/IRMinformationspatient.csv'  # Remplacer par le chemin souhaité pour le nouveau fichier CSV\n",
        "\n",
        "# Enregistrer les colonnes sélectionnées dans le nouveau fichier CSV\n",
        "colonnes_selectionnees.to_csv(nouveau_fichier_csv, index=False)\n",
        "\n",
        "print(f\"Nouveau fichier CSV créé : {nouveau_fichier_csv}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "138d218f",
      "metadata": {
        "id": "138d218f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "# Chemin vers le dossier contenant les images\n",
        "dossier_images = \"/Users/esmahannachi/downloads/IRMIMAGE\"\n",
        "\n",
        "# Chemin vers le fichier CSV contenant les identifiants des patients et les maladies\n",
        "fichier_csv = \"/Users/esmahannachi/downloads/IRMinformationspatient.csv\"\n",
        "\n",
        "# Créer un ensemble pour stocker les identifiants des images existantes\n",
        "identifiants_existants = set()\n",
        "\n",
        "# Parcourir les fichiers dans le dossier des images et remplir l'ensemble avec les identifiants existants\n",
        "for nom_fichier in os.listdir(dossier_images):\n",
        "    # Vérifier si le fichier est un fichier image valide (avec l'extension .nii)\n",
        "    if nom_fichier.endswith('.nii'):\n",
        "        # Extraire l'identifiant du patient à partir du nom du fichier\n",
        "        identifiant_patient = nom_fichier.split('.')[0]\n",
        "        identifiants_existants.add(identifiant_patient)\n",
        "\n",
        "# Créer une liste pour stocker les lignes à conserver\n",
        "lignes_a_conserver = []\n",
        "\n",
        "# Lire les identifiants des patients et les maladies à partir du fichier CSV\n",
        "with open(fichier_csv, 'r') as fichier:\n",
        "    lecteur_csv = csv.reader(fichier)\n",
        "    # Ajouter l'en-tête à lignes_a_conserver\n",
        "    lignes_a_conserver.append(next(lecteur_csv))\n",
        "    for ligne in lecteur_csv:\n",
        "        identifiant_patient = ligne[0]\n",
        "        # Vérifier si l'identifiant du patient existe dans l'ensemble des identifiants existants\n",
        "        if identifiant_patient in identifiants_existants:\n",
        "            # Si l'identifiant du patient existe, ajouter la ligne à lignes_a_conserver\n",
        "            lignes_a_conserver.append(ligne)\n",
        "\n",
        "# Écrire les lignes filtrées dans un nouveau fichier CSV\n",
        "fichier_sortie = \"/Users/esmahannachi/downloads/IRMinformationFinale.csv\"\n",
        "with open(fichier_sortie, 'w', newline='') as fichier:\n",
        "    writer_csv = csv.writer(fichier)\n",
        "    writer_csv.writerows(lignes_a_conserver)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "191394e7",
      "metadata": {
        "id": "191394e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "887c2ffa-de92-4492-8311-eeebcdeacf4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 60.80% \n",
            "Precision: 61.41%\n",
            "Specificity: 53.97%\n",
            "Recall/Sensitivity: 69.78%\n",
            "F1 Score: 63.59%\n",
            "AUC: 62.37%\n"
          ]
        }
      ],
      "source": [
        "import os  # Importer le module os pour travailler avec le système d'exploitation\n",
        "import numpy as np  # Importer numpy pour le calcul numérique\n",
        "import pandas as pd  # Importer pandas pour la manipulation des données\n",
        "import nibabel as nib  # Importer nibabel pour le chargement des images IRM\n",
        "import torch  # Importer PyTorch pour l'apprentissage profond\n",
        "import torch.nn as nn  # Importer les modules de réseau de neurones de PyTorch\n",
        "import torch.optim as optim  # Importer les modules d'optimisation de PyTorch\n",
        "from torch.utils.data import Dataset, DataLoader  # Importer les classes de jeu de données et de chargeur de données de PyTorch\n",
        "from sklearn.model_selection import train_test_split  # Importer train_test_split pour diviser les données\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, confusion_matrix  # Importer les métriques d'évaluation du modèle\n",
        "import torch.nn.functional as F  # Importer les fonctions utiles de PyTorch\n",
        "\n",
        "# Étape 1 : Charger le fichier CSV\n",
        "csv_path = os.path.join('/content/IRMformationFinal.csv')  # Chemin du fichier CSV contenant les informations des patients\n",
        "df = pd.read_csv(csv_path)  # Lire le fichier CSV dans un DataFrame pandas\n",
        "\n",
        "# Mapper les étiquettes aux entiers\n",
        "df['Group'] = df['Group'].map({'AD': 0, 'CN': 1})  # Mapper les étiquettes de groupe à des entiers pour l'apprentissage\n",
        "\n",
        "# Étape 2 : Prétraiter les images IRM\n",
        "image_folder = \"/content/drive/MyDrive/IRMIMAGE\"  # Chemin du dossier contenant les images IRM\n",
        "\n",
        "def load_image(patient_id, image_folder, target_shape=(256, 256, 166)):\n",
        "    # Fonction pour charger une image IRM et la prétraiter\n",
        "    image_path = os.path.join(image_folder, f'{patient_id}.nii')  # Chemin de l'image IRM\n",
        "    img = nib.load(image_path).get_fdata()  # Charger l'image IRM avec nibabel\n",
        "    # Redimensionner ou recadrer l'image à la taille cible\n",
        "    img = F.interpolate(torch.tensor(img).unsqueeze(0).unsqueeze(0), size=target_shape, mode='trilinear', align_corners=False)\n",
        "    img = img.squeeze().numpy()  # Convertir l'image en tableau numpy\n",
        "    # Effectuer un prétraitement si nécessaire (par exemple, normalisation)\n",
        "    # Renvoyer l'image prétraitée\n",
        "    return img\n",
        "\n",
        "# Définir une classe de jeu de données personnalisée\n",
        "class MRIDataset(Dataset):\n",
        "    def __init__(self, df, image_folder):\n",
        "        self.df = df  # DataFrame contenant les informations des patients\n",
        "        self.image_folder = image_folder  # Dossier contenant les images IRM\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)  # Retourner la taille du jeu de données\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        patient_id = self.df.iloc[idx]['Subject']  # Identifiant du patient à l'indice donné\n",
        "        label = self.df.iloc[idx]['Group']  # Étiquette du patient à l'indice donné\n",
        "        image = load_image(patient_id, self.image_folder)  # Charger l'image IRM du patient\n",
        "        # Convertir l'image et l'étiquette en tenseurs PyTorch\n",
        "        image = torch.tensor(image, dtype=torch.float32)  # Convertir l'image en tenseur\n",
        "        label = torch.tensor(label, dtype=torch.long)  # Convertir l'étiquette en tenseur\n",
        "        return image, label  # Retourner l'image et l'étiquette\n",
        "\n",
        "# Instancier le jeu de données\n",
        "dataset = MRIDataset(df, image_folder)  # Créer une instance de la classe MRIDataset\n",
        "\n",
        "# Diviser les données en ensembles d'entraînement et de validation\n",
        "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)  # Diviser le jeu de données\n",
        "\n",
        "# Créer des chargeurs de données\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Chargeur de données pour l'entraînement\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)  # Chargeur de données pour la validation\n",
        "\n",
        "# Étape 3 : Définir et entraîner un modèle d'apprentissage profond\n",
        "# Définir le modèle\n",
        "class MRIModelV2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MRIModelV2, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(1, 32, kernel_size=(3, 3, 3))\n",
        "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3))\n",
        "        self.conv3 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3))\n",
        "        self.conv4 = nn.Conv3d(128, 256, kernel_size=(3, 3, 3))\n",
        "        self.pool = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
        "        self.dropout = nn.Dropout3d(p=0.5)\n",
        "        self.fc1 = nn.Linear(256 * 5 * 5 * 5, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1, 256 * 5 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# Instancier le modèle\n",
        "model = MRIModelV2()  # Créer une instance du modèle\n",
        "\n",
        "# Définir la fonction de perte et l'optimiseur\n",
        "criterion = nn.BCELoss()  # Fonction de perte - Cross-Entropy binaire\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimiseur Adam avec un taux d'apprentissage de 0.001\n",
        "\n",
        "# Entraîner le modèle\n",
        "for epoch in range(10):\n",
        "    model.train()  # Mode entraînement\n",
        "    running_loss = 0.0  # Initialiser la perte en cours\n",
        "    for images, labels in train_loader:  # Boucle sur les données d'entraînement\n",
        "        optimizer.zero_grad()  # Remettre à zéro les gradients\n",
        "        outputs = model(images.unsqueeze(1))  # Passer les images au modèle\n",
        "        loss = criterion(outputs, labels.unsqueeze(1).float())  # Calculer la perte\n",
        "        loss.backward()  # Rétropropagation\n",
        "        optimizer.step()  # Mise à jour des poids\n",
        "        running_loss += loss.item()  # Ajouter la perte actuelle\n",
        "    print(f'Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader)}')  # Afficher la perte moyenne\n",
        "\n",
        "# Étape 4 : Évaluer le modèle entraîné\n",
        "model.eval()  # Mettre le modèle en mode évaluation\n",
        "predicted_labels = []  # Liste pour les étiquettes prédites\n",
        "true_labels = []  # Liste pour les étiquettes réelles\n",
        "with torch.no_grad():  # Désactiver le calcul des gradients\n",
        "    for images, labels in val_loader:  # Boucle sur les données de validation\n",
        "        outputs = model(images.unsqueeze(1))  # Passer les images au modèle\n",
        "        predicted = torch.round(outputs).squeeze(1)  # Arrondir les prédictions\n",
        "        predicted_labels.extend(predicted.tolist())  # Ajouter les prédictions à la liste\n",
        "        true_labels.extend(labels.tolist())  # Ajouter les étiquettes réelles à la liste\n",
        "\n",
        "# Convertir les listes en tableaux numpy pour les calculs supplémentaires\n",
        "predicted_labels = np.array(predicted_labels)  # Convertir les étiquettes prédites en tableau numpy\n",
        "true_labels = np.array(true_labels)  # Convertir les étiquettes réelles en tableau numpy\n",
        "\n",
        "# Calculer les métriques de performance\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)  # Calculer l'exactitude\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)  # Calculer la matrice de confusion\n",
        "precision = precision_score(true_labels, predicted_labels)  # Calculer la précision\n",
        "recall = recall_score(true_labels, predicted_labels)  # Calculer le rappel\n",
        "f1 = f1_score(true_labels, predicted_labels)  # Calculer le score F1\n",
        "auc = roc_auc_score(true_labels, predicted_labels)  # Calculer l'AUC\n",
        "\n",
        "# Afficher les métriques de performance\n",
        "print(f'Validation Accuracy: {accuracy * 100}%')  # Afficher l'exactitude\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')  # Afficher la matrice de confusion\n",
        "print(f'Precision: {precision}')  # Afficher la précision\n",
        "print(f'Recall: {recall}')  # Afficher le rappel\n",
        "print(f'F1 Score: {f1}')  # Afficher le score F1\n",
        "print(f'AUC: {auc}')  # Afficher l'AUC\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}